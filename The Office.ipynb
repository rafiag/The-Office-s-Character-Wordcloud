{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"The Office.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPaxENhvUJ61Ljw85MI56nH"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"O1LBuEifD8_N","colab_type":"text"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"sovW67dtD_B-","colab_type":"text"},"source":["## Import libraries"]},{"cell_type":"code","metadata":{"id":"88cAPZOuDlb1","colab_type":"code","colab":{}},"source":["# Basic libraries\n","import os\n","import pandas as pd\n","import numpy as np\n","from pprint import pprint\n","import warnings\n","warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n","import json\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Tokenize\n","import re\n","import nltk\n","from nltk.tokenize import RegexpTokenizer\n","nltk.download('punkt')\n","# Stopword Removal\n","import spacy\n","from spacy.lang.id.stop_words import STOP_WORDS\n","!python3 -m spacy download en\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# Vizualization\n","import PIL\n","from PIL import Image\n","from wordcloud import WordCloud, ImageColorGenerator\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PHuIDYwVEBcr","colab_type":"text"},"source":["## Define funtion"]},{"cell_type":"code","metadata":{"id":"R3MHgNgoECha","colab_type":"code","colab":{}},"source":["stop_words = list(stopwords.words('english'))\n","stop_words.extend(['well', 'yeah', 'know', 'okay'])\n","CONTRACTION_MAP = { \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n","\n","def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n","    \n","    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n","                                      flags=re.IGNORECASE|re.DOTALL)\n","    def expand_match(contraction):\n","        match = contraction.group(0)\n","        first_char = match[0]\n","        expanded_contraction = contraction_mapping.get(match)\\\n","                                if contraction_mapping.get(match)\\\n","                                else contraction_mapping.get(match.lower())                       \n","        expanded_contraction = first_char+expanded_contraction[1:]\n","        return expanded_contraction\n","        \n","    expanded_text = contractions_pattern.sub(expand_match, text)\n","    expanded_text = re.sub(\"'\", \"\", expanded_text)\n","    return expanded_text\n","\n","expand_contractions(\"Y'all can't expand contractions I'd think\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otBsB6JKEC1Z","colab_type":"text"},"source":["## Load data"]},{"cell_type":"code","metadata":{"id":"gmriih4dEFFo","colab_type":"code","colab":{}},"source":["root = '/The Office'\n","\n","listdir = [f for f in os.listdir(root+'/data/')]\n","print(len(listdir))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5r_hbBZ9E5K8","colab_type":"text"},"source":["# Get each character line"]},{"cell_type":"code","metadata":{"id":"4Ed-gMzIFq_e","colab_type":"code","colab":{}},"source":["character_list = {'Michael':{'mask':'Michael.png', 'lines':[]},\n","                  'Dwight':{'mask':'Dwight.png', 'lines':[]},\n","                  'Jim':{'mask':'Jim.png', 'lines':[]},\n","                  'Pam':{'mask':'Pam.png', 'lines':[]},\n","                  'Ryan':{'mask':'Ryan.png', 'lines':[]},\n","                  'Andy':{'mask':'Andy.png', 'lines':[]},\n","                  'Stanley':{'mask':'Stanley.png', 'lines':[]},\n","                  'Kevin':{'mask':'Kevin.png', 'lines':[]},\n","                  'Meredith':{'mask':'Meredith.png', 'lines':[]},\n","                  'Phyllis':{'mask':'Phyllis.png', 'lines':[]},\n","                  'Angela':{'mask':'Angela.png', 'lines':[]},\n","                  'Oscar':{'mask':'Oscar.png', 'lines':[]},\n","                  'Kelly':{'mask':'Kelly.png', 'lines':[]},\n","                  'Toby':{'mask':'Toby.png', 'lines':[]},\n","                  'Creed':{'mask':'Creed.png', 'lines':[]},\n","                  }\n","\n","for character in list(character_list.keys()):\n","    for f in listdir:\n","        with open(root+'/data/{0}' .format(f)) as data_file:    \n","            data = json.load(data_file)\n","            # print('Parsing:', f)\n","            data_file.close()\n","\n","        for scene in data['scenes']:\n","            for line in scene:\n","                if character in line['character']:\n","                    l = re.sub('[\\(\\[].*?[\\)\\]]', ' ', line['line']).lower() # Remove stage instruction eg. []\n","                    l = re.sub(' +', ' ', l).strip() # Remove double space and trailing whitespace\n","                    l = expand_contractions(l) # Expand shortened word\n","                    l = re.sub('[^a-zA-Z\\s]', '', l) # Remove special character and digit\n","                    l =  [token for token in l.split() if token not in stop_words and len(token)>3]\n","                    l = ' '.join(l)\n","                    if l: character_list[character]['lines'].append(l)\n","\n","# print(json.dumps(data, sort_keys=True, indent=4))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXwXCvF6E3Qx","colab_type":"code","colab":{}},"source":["for character in list(character_list.keys()):\n","    text = ' '.join(character_list[character]['lines'])\n","\n","    char_mask = np.array(Image.open(root+\"/mask/{0}.png\" .format(character)))\n","    image_colors = ImageColorGenerator(char_mask)\n","\n","    # Create and generate a word cloud image:\n","    wordcloud = WordCloud(stopwords=stop_words, background_color=\"white\").generate(text)\n","\n","    # Display the generated image:\n","    # plt.imshow(wordcloud, interpolation='bilinear')\n","\n","    wc = WordCloud(background_color=\"white\", width=400, height=400, mask=char_mask).generate(text)\n","    plt.imshow(wc.recolor(color_func=image_colors))\n","\n","    print(character)\n","    plt.axis(\"off\")\n","    plt.show()"],"execution_count":0,"outputs":[]}]}